# HHmodel--research
This is an experiment in a writing class for first-year PhD students, aiming at integrating the traditional methods of close reading with electronic methods. But it turned out to be an self-experimentation examining my own path dependency, testing just how quickly I give up on new approaches after I hit a snag.   

This project was originally conceived to explore the generation of Hodgkin-Huxley Model, the most important model of modern neurophysiology, through the role of the non-human participant squids. It fits well with my usual interest in the real biological foundations of artificial intelligence. And it also fits with my past academic training in environmental history.        

But in the end it became a paper on too much linguistics, physics and math. The first consequence of this was that I lost my cohort as readers; it seemed a bit too unreadable and difficult to understand. I realized that math can only be used as a DH tool, and once it becomes a subject for digital study, even if it's just to talk about how math proves that democratic voting doesn't work, it's alienating. As well as I eventually realized that I still preferred the half-science, half-Hogwarts vibe of biology, and that physics and math were too rigorous and hard-science. So it's still a good program that at least tells me what I don't like.

Through the iterations of the semester, I learned to map Social Network Analysis, while I didn't practice it enough this semester and soon found out I didn't have to draw it myself (SAD), it was very useful and may be better used later in analyzing post-war Japanese-American academic networks, where node and edge data is more hidden than the transatlantic one, for now.     

Another inspiring skill is to plot line graphs using quantitative methods based on text analysis. In fact, in traditional research methods, explaining why a paper takes a certain period of time as its research period is an important and often under-answered question. Text mining provides another perspective on the answer.（I think it's much more meaningful than word clouds.）Similar to Google's Books Ngram Viewer, it is great for identifying approximate historical staging before research. 

Then comes the last but not least - text mining. But more than the subsequent work, the first thing for my research subject is to convert printed text into free doc, which means convert the archive into data. During this semester I found a pretty good OCR tool（https://github.com/getomni-ai/zerox?tab=readme-ov-file#python-zerox), which does a good job at recognizing printed text and outputting structured text. Even though it can't handle handwriting at all. But the not-so-bad news is that I don't have a lot of handwriting to deal with either. Thankfully, I chose Modern History for myself.      

